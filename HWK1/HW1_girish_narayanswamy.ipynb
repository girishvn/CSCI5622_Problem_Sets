{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - KNN - Girish Narayanswamy\n",
    "## CSCI 5622 - Spring 2019\n",
    "\n",
    "For today's assignment, we will be implementing our own K-Nearest Neighbors (KNN) algorithm.\n",
    "\n",
    "*But Professor Quigley, hasn't someone else already written KNN before?*\n",
    "\n",
    "Yes, you are not the first to implement KNN, or basically any algorithm we'll work with in this class. But 1) I'll know that you know what's really going on, and 2) you'll know you can do it, because 2a) someday you might have to implement some machine learning algorithm from scratch - maybe for a new platform (do you need to run python on your SmartToaster just to get it to learn how users like their toast?), maybe because you want to tweak the algorithm (there's always a better approach...), or maybe because you're working on something important and you need to control exactly what's on there (should you really be running anaconda on your secret spy plane?).\n",
    "\n",
    "That said - we're not going to implement *everything*. We'll start by importing a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "# test imports - unused\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wait a minute - didn't we just import Scikit-learn (sklearn)? The package with baked-in machine learning tools?*\n",
    "\n",
    "Yes - but it also has a ton of helper functions, including a dataset we'll be using later. But, for now, let's set up a KNNClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "class KNNClassifier:\n",
    "    \n",
    "    def __init__(self, X, y, k = 5):\n",
    "        \"\"\"\n",
    "        Initialize our custom KNN classifier\n",
    "        PARAMETERS\n",
    "        X - our training data features\n",
    "        y - our training data answers\n",
    "        k - the number of nearest neighbors to consider for classification\n",
    "        \"\"\"\n",
    "        self._model = sklearn.neighbors.BallTree(X)\n",
    "        self._y = y\n",
    "        self._k = k\n",
    "        self._counts = self.getCounts()\n",
    "        \n",
    "    def getCounts(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary storing the counts of each answer class found in y\n",
    "        RETURNS\n",
    "        counts - a dictionary of counts of answer classes\n",
    "        \"\"\"\n",
    "        \n",
    "        counts = dict({1:0,-1:0})\n",
    "        #BEGIN Workspace 1.1\n",
    "        #TODO: Modify and/or add to counts so that it returns a count of each answer class found in y\n",
    "        \n",
    "        vals, val_counts = np.unique(self._y, return_counts = True)\n",
    "        counts = dict(zip(vals.tolist(), val_counts.tolist()))\n",
    "        \n",
    "        #END Workspace 1.1\n",
    "        return(counts)\n",
    "    \n",
    "    def majority(self, indices):\n",
    "        \"\"\"\n",
    "        Given indices, report the majority label of those points.\n",
    "        For a tie, report the most common label in the data set.\n",
    "        PARAMETERS\n",
    "        indices - an np.array, where each element is an index of a neighbor\n",
    "        RETURNS\n",
    "        label - the majority label of our neighbors\n",
    "        \"\"\"\n",
    "        label = 0\n",
    "        #BEGIN Workspace 1.2\n",
    "        #TODO: Determine majority, assign it to label\n",
    "        \n",
    "        label_vals, label_counts = np.unique(np.array(((self._y).flatten())[indices]), return_counts=True)  # grabs labels and their occurrences\n",
    "        max_label_count = np.max(label_counts)  # maximum occurrence value\n",
    "        tie_indexes = (np.argwhere(label_counts == max_label_count)).flatten()  # grabs indexs of all labels with max occurrence\n",
    "\n",
    "        label = label_vals[tie_indexes[0]]  # init output as first of tied labels\n",
    "        tie_break_label_count = label_counts[tie_indexes[0]]\n",
    "\n",
    "        if tie_indexes.size > 1:  # if tie exists\n",
    "            for i in label_vals[tie_indexes]:  # iterate through all label options\n",
    "                if self._counts[i] > tie_break_label_count:  # check their counts in the whole training set\n",
    "                    label = i  # if occur more often switch label\n",
    "        \n",
    "        \n",
    "        #END Workspace 1.2\n",
    "        return(label)\n",
    "    \n",
    "    def classify(self, point):\n",
    "        \"\"\"\n",
    "        Given a new data point, classify it according to the training data X and our number of neighbors k into the appropriate class in our training answers y\n",
    "        PARAMETERS\n",
    "        point - a feature vector of our test point\n",
    "        RETURNS\n",
    "        ans - our predicted classification\n",
    "        \"\"\"\n",
    "        ans = 0\n",
    "        #BEGIN Workspace 1.3\n",
    "        #TODO: perform classification of point here\n",
    "        #HINT: use the majority function created above\n",
    "        #HINT: use the euclidian distance discussed in lecture to find nearest neighbors\n",
    "        \n",
    "        distances, indices = self._model.query(point.reshape((1, -1)), k = self._k)\n",
    "        ans = self.majority(indices)\n",
    "        \n",
    "        #END Workspace 1.3\n",
    "        return(ans)\n",
    "    \n",
    "    def confusionMatrix(self, testX, testY):\n",
    "        \"\"\"\n",
    "        Generate a confusion matrix for the given test set\n",
    "        PARAMETERS\n",
    "        testX - an np.array of feature vectors of test points\n",
    "        testY - the corresponding correct classifications of our test set\n",
    "        RETURN\n",
    "        C - an N*N np.array of counts, where N is the number of classes in our classifier\n",
    "        \"\"\"\n",
    "        #C = np.array() # modified below\n",
    "        N = len(self._counts)\n",
    "        C = np.zeros((N, N))\n",
    "        \n",
    "        #BEGIN Workspace 1.4\n",
    "        #TODO: Run classification for the test set, compare to test answers, and add counts to matrix\n",
    "        \n",
    "        # the following is down to account for negative labels, and sparse label values (non-continuous)\n",
    "        labels_list = np.fromiter(self._counts.keys(), dtype=float)  # list of all labels\n",
    "        \n",
    "        # 3.1 usage\n",
    "        confusedIndx = [] # used to iterate through the test images \n",
    "        confusedLabel = [] # used to show what the classifier thought the label is\n",
    "        \n",
    "        # iterate through test set \n",
    "        for i in range(0, testY.size - 1): # iterate through number test points\n",
    "            \n",
    "            test_label = self.classify(testX[i]) # grab the classification of test point x\n",
    "            actual_label = (testY.flatten())[i] # grab the actual label\n",
    "            \n",
    "            test_label_index = (np.argwhere(labels_list == test_label)).flatten() # find the index of that label in the labels list\n",
    "            actual_label_index = (np.argwhere(labels_list == actual_label)).flatten() # find the index of that label in the labels list\n",
    "            \n",
    "            C[test_label_index[0], actual_label_index[0]] += 1 # iterate the index [test_label_index, actual_label_index] by 1 in the confusion matrix\n",
    "            \n",
    "            # 3.1 usage\n",
    "            if test_label != actual_label:\n",
    "                confusedIndx.append(i)\n",
    "                confusedLabel.append(test_label)\n",
    "                      \n",
    "        #END Workspace 1.4\n",
    "        return(C, confusedIndx, confusedLabel)\n",
    "    \n",
    "    def accuracy(self, C):\n",
    "        \"\"\"\n",
    "        Generate an accuracy score for the classifier based on the confusion matrix\n",
    "        PARAMETERS\n",
    "        C - an np.array of counts\n",
    "        RETURN\n",
    "        score - an accuracy score\n",
    "        \"\"\"\n",
    "        score = np.sum(C.diagonal()) / C.sum()\n",
    "        return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But professor, this code isn't complete!*\n",
    "\n",
    "### Problem 1: Complete our KNN Classifier - 40 Points (10 each)\n",
    "\n",
    "1.1 - Complete the getCounts function to return the count of each class found in the training set\n",
    "\n",
    "1.2 - Complete the majority function to determine the majority class of a series of neighbors\n",
    "\n",
    "1.3 - Complete the classify function to capture the predicted class of a new datapoint\n",
    "\n",
    " - HINT: Use the BallTree documentation to determine how to retrieve neighbors from the model (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree)\n",
    "\n",
    "1.4 - Complete the confusionMatrix function to reveal the results of classification\n",
    "\n",
    "You can take a look at the unit tests below to see how we create data to input into our classifier, what kinds of things we expect as output, etc. You should also consider expanding the test cases to make sure your classifier is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class KNNTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.x = np.array([[3,1],[2,8], [2,7], [5,2],[3,2],[8,2],[2,4]])\n",
    "        self.y = np.array([[1, -1, -1, 1, -1, 1, -1]])\n",
    "        self.knnfive = KNNClassifier(self.x, self.y)\n",
    "        self.knnthree = KNNClassifier(self.x, self.y, 3)\n",
    "        self.knnone = KNNClassifier(self.x, self.y, 1)\n",
    "        \n",
    "        self.testPoints = np.array([[2,1], [2,6], [4, 4]])\n",
    "        \n",
    "    def testCounter(self):\n",
    "        \"\"\"\n",
    "        Test getCounts function from knnclassifier\n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knnfive._counts[1], 3)\n",
    "        self.assertEqual(self.knnfive._counts[-1], 4)\n",
    "        \n",
    "    def testKNNOne(self):\n",
    "        \"\"\"\n",
    "        Test if the classifier returns \"correct\" (expected) classifications for k = 1\n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knnone.classify(self.testPoints[0]), 1)\n",
    "        #BEGIN Workspace\n",
    "        #Add more tests as needed\n",
    "        #END Workspace\n",
    "    \n",
    "    #BEGIN Workspace\n",
    "    #Add more test functions as requested\n",
    "    def testConfusionMtx(self):\n",
    "        \"\"\"\n",
    "        Test if the confusion matrix is properly built for knn 5\n",
    "        \"\"\"\n",
    "        testX = np.array([[3,1],[2,8], [2,7], [5,2],[3,2],[8,2],[2,4]])\n",
    "        testY = np.array([[1, -1, -1, 1, -1, 1, -1]])\n",
    "        \n",
    "        \n",
    "        C = KNNClassifier.confusionMatrix(self.knnfive, testX, testY)[0]\n",
    "        score = KNNClassifier.accuracy(self.knnfive, C)\n",
    "        \n",
    "        \n",
    "        self.assertEqual(C[0][0], 2)\n",
    "        self.assertEqual(C[0][1], 0)\n",
    "        self.assertEqual(C[1][0], 1)\n",
    "        self.assertEqual(C[1][1], 3)\n",
    "    \n",
    "    #HINT - You'll want to make sure your\n",
    "    #END Workspace\n",
    "    \n",
    "tests = KNNTester()\n",
    "myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - now we've demonstrated that our KNN classifier works, let's think about our problem space! \n",
    "\n",
    "## Our Dataset - Identifying Digits from Images\n",
    "\n",
    "It's a pretty common problem - just imagine working at the post office, or at a bank, and you're handed a hand-written envelope, or check, or other piece of information and you have to identify exactly what it says. Did they pay 500 or 600 dollars? Is the letter going to 80309 (campus) or 30309 (Atlanta)?\n",
    "\n",
    "Let's be a little smart about this - let's up some classes and helper functions to help us out.\n",
    "\n",
    "### Problem 2: Implement KNN on Digits dataset - 30 Points\n",
    "\n",
    "2.1 Randomly divide our Digits dataset into training and testing sets (15 Points)\n",
    "\n",
    "2.2 Report the number of examples in training and testing, as well as measuring then number of pixels in each image (5 points)\n",
    "\n",
    "2.3 Create a confusion matrix of our classifier for K = 5 (10 points) *HINT: Doing this may cause you to catch mistakes in your classifier. Go fix those!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Numbers:\n",
    "    def __init__(self):\n",
    "        #load data from sklearn\n",
    "        digits = sklearn.datasets.load_digits()\n",
    "        \n",
    "        #BEGIN Workspace 2.1\n",
    "        m, n = digits.data.shape\n",
    "        idx = np.random.permutation(m) # grab a random perm from 0-m\n",
    "        range_train = range(0,round(0.8*m)) # 80 percent split\n",
    "        range_test = range(round(0.8*m), m) # 20 percent split\n",
    "        \n",
    "        self.train_x = np.array(digits.data[range_train]) # A 2D np.array of training examples\n",
    "        self.train_y = np.array(digits.target[range_train]) # A 1D np.array of training answers\n",
    "        self.test_x = np.array(digits.data[range_test]) # A 2D np.array of testing examples\n",
    "        self.test_y = np.array(digits.target[range_test]) # A 1D np.array of testing answers\n",
    "        \n",
    "        # used for problem 3.1\n",
    "        self.test_imgs = digits.images[range_test]\n",
    "\n",
    "        # Alternately... \n",
    "        #self.train_x, self.test_x, self.train_y, self.test_y = sklearn.model_selection.train_test_split(digits.data, digits.target, test_size=0.20)\n",
    "        \n",
    "        #self.train_x = np.array() # A 2D np.array of training examples, REPLACE\n",
    "        #self.train_y = np.array() # A 1D np.array of training answers, REPLACE\n",
    "        #self.test_x = np.array() # A 2D np.array of testing examples, REPLACE\n",
    "        #self.test_y = np.array() # A 1D np.array of testing answers, REPLACE\n",
    "        #TODO: Divide our dataset into Train and Test datasets (80/20 split), replacing the variables above\n",
    "         \n",
    "        #END Workspace 2.1\n",
    "        \n",
    "    def report(self):\n",
    "        \"\"\"\n",
    "        Report information about the dataset using the print() function\n",
    "        \"\"\"\n",
    "        #BEGIN Workspace 2.2\n",
    "        #TODO: Create printouts for reporting the size of each set and the size of each datapoint\n",
    "        \n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Training and Test Dataset Report:\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"train_x size:\", self.train_x.shape[0])\n",
    "        print(\"train_y size:\", self.train_y.shape[0])\n",
    "        print(\"test_x size:\", self.test_x.shape[0])\n",
    "        print(\"test_y size:\", self.test_y.shape[0])\n",
    "        print(\"pixels per data point:\", self.train_x.shape[1])\n",
    "        print(\"\")\n",
    "        \n",
    "        #END Workspace 2.2\n",
    "        \n",
    "\n",
    "    def classify(self):\n",
    "        \"\"\"\n",
    "        Create a classifier using the training data and generate a confusion matrix for the test data\n",
    "        \"\"\"\n",
    "        #BEGIN Workspace 2.3\n",
    "        #TODO: Create classifier from training data, generate confusion matrix for test data\n",
    "        \n",
    "        self.knnInst = KNNClassifier(self.train_x, self.train_y,self.k)        \n",
    "        C, confusedIndx, confusedLabel = KNNClassifier.confusionMatrix(self.knnInst, self.test_x, self.test_y)\n",
    "        score = KNNClassifier.accuracy(self.knnInst, C)\n",
    "        \n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Classifier Report:\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(C, \"Confusion Matrix\")\n",
    "        print(\"\")\n",
    "        print(score, \"Confusion Score\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # 3.1 usage\n",
    "        for i in range(len(confusedIndx)):\n",
    "            self.viewDigit(self.test_imgs[confusedIndx[i]])\n",
    "            print(\"Confused for:\", confusedLabel[i])\n",
    "            print(\"Actually:\", self.test_y[confusedIndx[i]])\n",
    "            print(\"\")\n",
    "        \n",
    "        #END Workspace 2.3\n",
    "        \n",
    "    def viewDigit(self, digitImage):\n",
    "        \"\"\"\n",
    "        Display an image of a digit\n",
    "        PARAMETERS\n",
    "        digitImage - a data object from the dataset\n",
    "        \"\"\"\n",
    "        plt.gray()\n",
    "        plt.matshow(digitImage)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These unit tests are not run because 3.1 effectively checks all these things \n",
    "\n",
    "import unittest\n",
    "\n",
    "class NumbersTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.numbers_test = Numbers()\n",
    "        self.numbers_test.k = 5\n",
    "        \n",
    "    def testReport(self):\n",
    "        \"\"\"\n",
    "        Test getCounts function from knnclassifier\n",
    "        \"\"\"\n",
    "        self.numbers_test.report()\n",
    "        \n",
    "    def testClassify(self):\n",
    "        \"\"\"\n",
    "        Test getCounts function from knnclassifier\n",
    "        \"\"\"\n",
    "        self.numbers_test.classify()\n",
    "        \n",
    "    def testViewDigit(self):\n",
    "        \"\"\"\n",
    "        Test viewDigit, and get sample images of digits 1-9\n",
    "        \"\"\"\n",
    "        for i in range(10):\n",
    "            self.numbers_test.viewDigit(sklearn.datasets.load_digits().images[i])\n",
    "    \n",
    "#tests = NumbersTester()\n",
    "#myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "#unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wow, I can't believe we just created a KNN Classifier - but can't we make it better?*\n",
    "\n",
    "Yes, we saw above that our classifier didn't work perfectly. Let's explore that issue a little further\n",
    "\n",
    "### Problem 3: Improving KNN on Digits - 30 Points\n",
    "\n",
    "3.1 Determine which classes are most often confused (from our confusion matrix above), inspect some examples of these digits (using the viewDigit function in our Numbers class), and write a brief (4 - 5 sentences) description of why you think these particular numbers may be misclassified.\n",
    "\n",
    "3.2 Explore the influence of the number of nearest neighbors (i.e. try changing our K). Plot the relationship between K and accuracy, and write a brief (4 - 5 sentences) description of how this factor impacts our accuracy.\n",
    "\n",
    "3.3 (Bonus) Explore the influence of the train / test split of our data (i.e. copy our Numbers class into Numbers2 below and try changing the split for our dataset). Plot the relationship between the split % and accuracy, and write a brief (4 - 5 sentences) description of its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN 3.1a\n",
    "#TODO: Print out problem class images\n",
    "\n",
    "# For a classification with 0.8 training set split, and 5 KNN\n",
    "# The Following code generates and prints the Confusion matrix and the accuracy score\n",
    "# It also prints the size report\n",
    "# Finally it prints all the confused images as well as the actual classification and what the images was labeled\n",
    "     \n",
    "test31 = Numbers()\n",
    "test31.k = 5\n",
    "test31.classify()\n",
    "\n",
    "#END 3.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1b\n",
    "TODO: Write description of misclassification\n",
    "\n",
    "The above output shows the misclassified images, as well as their labeled and actual classifications. Common images that are often confused seem to be 3, 7, 8, and 9. Either these values are confused for others, or other values are confused for these. These numbers may be misclassified often because they may be be fairly close to all image points in Euclidean space. Thus they often fall into the nearest neighbors of an image, and thus multiple image classifications may fall into their neighborhood.\n",
    "\n",
    "To be fair to the algorithm, for many of the images confused (displayed above) I could not correctly classify many of the image by eye. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numbers2:\n",
    "    def __init__(self, trainPercentage):\n",
    "        #load data from sklearn\n",
    "        digits = sklearn.datasets.load_digits()\n",
    "        \n",
    "        #BEGIN Workspace 3.3a\n",
    "        \n",
    "        m, n = digits.data.shape\n",
    "        idx = np.random.permutation(m) # grab a random perm from 0-m\n",
    "        range_train = range(0,round(trainPercentage*m)) # trainPercentage percent split\n",
    "        range_test = range(round(trainPercentage*m), m) # 1 - trainPercentage percent split\n",
    "        \n",
    "        self.train_x = np.array(digits.data[range_train]) # A 2D np.array of training examples\n",
    "        self.train_y = np.array(digits.target[range_train]) # A 1D np.array of training answers\n",
    "        self.test_x = np.array(digits.data[range_test]) # A 2D np.array of testing examples\n",
    "        self.test_y = np.array(digits.target[range_test]) # A 1D np.array of testing answers\n",
    "        \n",
    "        #self.train_x = np.array() # A 2D np.array of training examples, REPLACE\n",
    "        #self.train_y = np.array() # A 1D np.array of training answers, REPLACE\n",
    "        #self.test_x = np.array() # A 2D np.array of testing examples, REPLACE\n",
    "        #self.test_y = np.array() # A 1D np.array of testing answers, REPLACE\n",
    "        #TODO: Divide our dataset into Train and Test datasets (using trainPercentage), replacing the variables above\n",
    "        #HINT: You should be able to mostly copy your own work from the original Numbers class\n",
    "        #END Workspace 3.3a\n",
    "\n",
    "    def classify(self, k):\n",
    "        \"\"\"\n",
    "        Create a classifier using the training data and generate a confusion matrix for the test data\n",
    "        \"\"\"\n",
    "        #BEGIN Workspace 3.2a\n",
    "        #TODO: Create classifier from training data (using k nearest neighbors), generate confusion matrix for test data\n",
    "        #HINT: You can copy your own work from the original Numbers class\n",
    "        \n",
    "        self.knnInst = KNNClassifier(self.train_x, self.train_y, k)\n",
    "        C = KNNClassifier.confusionMatrix(self.knnInst, self.test_x, self.test_y)[0]\n",
    "        score = KNNClassifier.accuracy(self.knnInst, C)\n",
    "        \n",
    "        return score\n",
    "        \n",
    "        #END Workspace 3.2a\n",
    "        \n",
    "    def viewDigit(digitImage):\n",
    "        \"\"\"\n",
    "        Display an image of a digit\n",
    "        PARAMETERS\n",
    "        digitImage - a data object from the dataset\n",
    "        \"\"\"\n",
    "        plt.gray()\n",
    "        plt.matshow(digitImage)\n",
    "        plt.show()\n",
    "\n",
    "# PLEASE NOTE: THE GRAPHS TAKE A VERY LONG TIME TO GENERATE \n",
    "\n",
    "# 3.2 relationship of accuracy and k        \n",
    "test32 = Numbers2(0.8)\n",
    "scoreArray32 = np.zeros((2,len(test32.train_x)))\n",
    "for k in range(1,len(test32.train_x)):\n",
    "    scoreArray32[0][k] = k\n",
    "    scoreArray32[1][k] = test32.classify(k)\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.scatter(scoreArray32[0,:], scoreArray32[1,:])\n",
    "plt.title(\"Accuracy Score as a Factor of k Nearest Neighbors\")\n",
    "plt.xlabel(\"Number of Nearest Neighbors (k)\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()\n",
    "\n",
    "# 3.3 relationship of accuracy and split   \n",
    "scoreArray33 = np.zeros((2,10)) # check 10 different splits \n",
    "for split in range(1,10):\n",
    "    \n",
    "    test33 = Numbers2(split/10)\n",
    "    scoreArray33[0][split] = (split/10)\n",
    "    scoreArray33[1][split] = test33.classify(100)\n",
    "    \n",
    "plt.figure(2)\n",
    "plt.scatter(scoreArray33[0,:], scoreArray33[1,:])\n",
    "plt.title(\"Accuracy Score as a Factor of Percentage Split of Training Set\")\n",
    "plt.xlabel(\"Percentage Split of Training Set\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2b\n",
    "TODO: Write description of influence of neighbor count\n",
    "\n",
    "The neighbor count k, has an inverse relationship with the accuracy score. As k increases the accuracy drops almost linearly. This is because, as images of similar label are close in Euclidean space, for lower values of k the nearest neighbors comprise of images with the same label as the image being classified. However when k starts to become large, large numbers of images are in the neighborhood that are of a different label. Even though these images may be far in Euclidean space, if they are in the k-nearest neighborhood they are weighed equal to those images close to the image being classified. As neighbors are not weighted in any way, according to their distance from the image being classified, as k increases the accuracy decreases.  \n",
    "\n",
    "#### 3.3b\n",
    "TODO: Write description of influence of training / testing split\n",
    "\n",
    "As the the training set percentage increases, the accuracy of the KNN classification increases similar to a log function. If the training set is small, relative to the test set, then there are few known values to classify images in the test set. This leads to poor accuracy. On the other hand, for large a training set, the nearest neighbors of an image being classified are much more densely populated around the image, and better represent the classification of that image. Thus as the training size percentage increases the accuracy also increases. The log-like growth of this trend implies that after a certain percentage the accuracy beings to plateau (at some relatively high value) for increasing percentage splits of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
